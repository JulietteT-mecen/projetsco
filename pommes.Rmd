---
title: "Étude de la production de pommes"
author: "Maurice Thomas & Thiret Juliette"
date: "30 octobre 2019"
output: html_document
---

```{r setup, include=FALSE,warning=FALSE, comment=NA, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r,echo=FALSE, message=FALSE,warning=FALSE}
require(data.table)
require(corrplot)
require(ggplot2)
require(lmtest)
require(miscTools)
require(stargazer)
```

```{r}
setwd("C:/Users/PDX/Desktop/Cours/M2/Mesure et perf/projetsco")
data=read.csv("Pommes.csv",sep=";",dec=",")
data=as.data.table(data)
```


#Manipulation de la base de données
```{r,warning=FALSE, comment=NA, message=FALSE}
data<-data[,-1]
```

```{r,warning=FALSE, comment=NA, message=FALSE}
summary(data)
str(data)
```

Toutes les variables sont bien codées.

```{r,warning=FALSE, comment=NA, message=FALSE, echo=TRUE}
data$adv<-as.factor(data$adv)
```


**Création de variables : les quantités de facteurs**

```{r,warning=FALSE, comment=NA, message=FALSE,echo=TRUE}
data$qCap<-(data$vCap/data$pCap)
data$qMat<-(data$vMat/data$pMat)
data$qLab<-(data$vLab/data$pLab)
```

Vérification du cout total

```{r,warning=FALSE, comment=NA,echo=TRUE}
#cout total
data[,cost:=vCap+vLab+vMat]
all.equal(data$cost, with( data, pCap * qCap + pLab * qLab + pMat * qMat )) 
#cout variable
data[,vCost:=vLab+vMat]

```


#Statistiques descriptives


On remarque que beaucoup de variables sont corrélées entre elles. Mais ces corrélations semblent logiques

**Etape 1: Histogrammes**

Représentez les histogrammes des productivités moyennes des facteurs de production de Cap,
Lab et Mat.

On considère donc les 3ieres colonnes

La productivité moyenne est : le nombre d'output / les moyens de production.
On créer donc une nouvelle colonne : vcap / qout par exemple.

```{r,warning=FALSE, comment=NA, message=FALSE}
data=as.data.table(data)
data=data[,pmcap := qOut/vCap]
```

```{r,warning=FALSE, comment=NA, message=FALSE}
data=as.data.table(data)
data=data[,pmlab := qOut/vLab]
```

```{r,warning=FALSE, comment=NA, message=FALSE}
data=as.data.table(data)
data=data[,pmmat := qOut/vMat]
```


**Histogrammes des productivités moyennes**
```{r,warning=FALSE, comment=NA, message=FALSE}
par(mfrow=c(1,1))
ggplot(data, aes(x=pmcap)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
ggplot(data, aes(x=pmlab)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
ggplot(data, aes(x=pmmat)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
```


**Etape 2** : Etude des corrélations.

Etudiez les corrélations entre les quantités des 3 facteurs de production utilisés dans ce secteur

Donnez la matrice des corrélations, et représentez graphiquement le nuage de point correspondant aux différents croisements des productivités moyennes prises deux à deux.
Représentez aussi les 3 séries de productivité moyenne par rapport à l'output total qOut.
###Matrice des corrélations
```{r,warning=FALSE, comment=NA, message=FALSE}
databis<-data[,c(1,2,3,6)]
M=cor(databis)
corrplot(M,type="lower", addCoef.col="red", method="shade")
```

###Nuage de point

```{r,warning=FALSE, comment=NA, message=FALSE}
par(mfrow=c(2,2))
ggplot(data, aes(x=pmlab, y=pmcap)) + 
  geom_point()+
  geom_smooth(method=lm)

ggplot(data, aes(x=pmlab, y=pmmat)) + 
  geom_point()+
  geom_smooth(method=lm)

ggplot(data, aes(x=pmcap, y=pmmat)) + 
  geom_point()+
  geom_smooth(method=lm)

```

```{r,warning=FALSE, comment=NA, message=FALSE}
attach(data)
par(mfrow=c(2,2))
ggplot(data, aes(x=pmlab, y=qOut)) + 
  geom_point()+
  geom_smooth(method=lm)

ggplot(data, aes(x=pmmat, y=qOut)) + 
  geom_point()+
  geom_smooth(method=lm)

ggplot(data, aes(x=pmcap, y=qOut)) + 
  geom_point()+
  geom_smooth(method=lm)

```


**Etape 3** : Calcul des indices de Pasches, Laspeyres et Fisher

Indice de valeur = indice de prix × indice de volume
indice de volume = indice de valeur / indice de prix


```{r,warning=FALSE, comment=NA, message=FALSE,echo=TRUE}
data[,XP:=(vCap+vLab+vMat) / (mean(qCap)*pCap + mean(qLab)*pLab + mean(qMat)*pMat)]
data[,XL:=(qCap*mean(pCap) + qLab*mean(pLab) + qMat*mean(pMat)) /
       (mean(qCap)*mean(pCap) + mean(qLab)*mean(pLab) + mean(qMat)*mean(pMat))]
data[,XF:=sqrt(XP*XL)]

```



**Etape 4** : Calcul d'un indice de productivité globale des facteurs


```{r,warning=FALSE, comment=NA,echo=TRUE}
data[,PGF:= qOut/XF]
```

**Etape 5** : Productivité globale et relations avec les quantités de facteurs

Représentez l'histogramme de l'indice de productivité globale des facteurs, sa relation avec l'output, sa relation avec les indices de quantité de facteur de production

```{r,warning=FALSE, comment=NA,echo=TRUE}
ggplot(data, aes(x=PGF)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
```

```{r,warning=FALSE, comment=NA, message=FALSE}
ggplot(data, aes(x=PGF, y=qOut)) + 
  geom_point()+
  geom_smooth(method=lm)
ggplot(data, aes(x=PGF, y=qCap)) + 
  geom_point()+
  geom_smooth(method=lm)
ggplot(data, aes(x=PGF, y=qLab)) + 
  geom_point()+
  geom_smooth(method=lm)
ggplot(data, aes(x=PGF, y=qMat)) + 
  geom_point()+
  geom_smooth(method=lm)
```

**Etape 6** : Boxplots

Représentez graphiquement (Boxplot) la productivité globale des facteurs pour les producteurs prenant conseil ou non.

```{r,warning=FALSE, comment=NA, message=FALSE}
#Box plots basiques
p <- ggplot(data, aes(x=adv, y=PGF,color=adv)) + 
  geom_boxplot()
p 
```

**Quelles sont les entreprises les plus performantes aux vues statistiques descriptives.**

=> Répondre 


#Estimation des fonctions de production linéaire


## Linéaire 

$$q_i = \alpha + \sum_{k=1}^{3} \beta_k x_ik + \epsilon_i$$

Avec une fonction de production linéaire on suppose implicitement que les facteurs de production sont des substituts. 

```{r,warning=FALSE, comment=NA, message=FALSE}
require(stargazer)
lm1<-lm(qOut~qLab+qMat+qCap, data=data)
stargazer(lm1, type="text")
```



Obtenez-vous une fonction de production monotone croissante ?

```{r,warning=FALSE, comment=NA, message=FALSE}
plot(lm1$fitted.values, main="production estimée par la RLS")# estimée
abline(lm1)
```

La production trouvée n'est pas monotone croissante. 

** Le coefficient associé au facteur de production du capital est il significatif ? **

La quantité de capitale n'est pas significative. Les autres quantités sont significatives au seuil de 1%. 

**Peut-on dire que chaque facteur de production est essentiel ?**

Oui chaque facteur est essentiel à la production ce n'est pas parce que le coefficient du capital n'est pas significatif que l'on doit en déduire cela. Il faut surtout estimer une autre fonction de production. 


**Comparer à l'aide d'un graphique la production observée et la production estimée **

```{r,warning=FALSE, comment=NA, message=FALSE}
compPlot(data$qOut, fitted(lm1))
sum(fitted(lm1) < 0 )
coef(lm1)
```


**Vérifier si la production est toujours positive **

```{r,warning=FALSE, comment=NA, message=FALSE}

sum(lm1$fitted.values< 0)
sort(lm1$fitted.values)[1:5]

```
Il y a une production estimée qui est négative

### Les élasticités

<ul>
<li>Calculez pour chaque firme l'élasticité de l'output par rapport à chacun des trois inputs capital,travail et matériaux.</li>
<li>Calculer la moyenne de ces élasticités sur l'échantillon.</li>
<li>Représentez la distribution empirique des ces élasticités. </li>
</ul>

$$\epsilon_i =  \frac{\partial f(x)}{\partial x_i} \frac {x_i}{f(x)} $$

```{r,message=FALSE, warning=FALSE, comment=NA}
data[,eps_cap:=coef(lm1)["qCap"]*qCap/qOut]
data[,eps_lab:=coef(lm1)["qLab"]*qCap/qOut]
data[,eps_mat:=coef(lm1)["qMat"]*qCap/qOut]

```

```{r}
# Elasticités
data[,eps_cap:=coef(lm1)["qCap"]*qCap/qOut]
data[,eps_lab:=coef(lm1)["qLab"]*qLab/qOut]
data[,eps_mat:=coef(lm1)["qMat"]*qMat/qOut]
colMeans(data[, c("eps_cap","eps_lab","eps_mat"), with=FALSE])
ggplot(data, aes(x=eps_cap)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
ggplot(data, aes(x=eps_lab)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
ggplot(data, aes(x=eps_mat)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")

# Comparaison avec la production estimée
data[,eps_cap_fit:=coef(lm1)["qCap"]*qCap/fitted(lm1)]
data[,eps_lab_fit:=coef(lm1)["qLab"]*qLab/fitted(lm1)]
data[,eps_mat_fit:=coef(lm1)["qMat"]*qMat/fitted(lm1)]
colMeans(data[, c("eps_cap_fit","eps_lab_fit","eps_mat_fit"), with=FALSE])
ggplot(data, aes(x=eps_cap_fit)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité estimée du capital")
ggplot(data, aes(x=eps_lab_fit)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité estimée du travail")
ggplot(data, aes(x=eps_mat_fit)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité estimée du matériel")

#Elasticités d'échelle
data[,eps_echelle:= eps_cap+eps_lab+eps_mat]
data[,eps_echelle_fit:= eps_cap_fit+eps_lab_fit+eps_mat_fit]
colMeans(data[, c("eps_echelle","eps_echelle_fit"), with=FALSE])
ggplot(data, aes(x=eps_echelle)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité d'échelle")

ggplot(data, aes(x=eps_echelle_fit)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité d'échelle estimée")
```

```{r,warning=FALSE, comment=NA, message=FALSE}
### Taux marginal de substitution technique
tmst_CapLab=-coef(lm1)["qLab"] / coef(lm1)["qCap"]
tmst_CapMat=-coef(lm1)["qMat"] / coef(lm1)["qCap"]
tmst_LabCap=-coef(lm1)["qCap"] / coef(lm1)["qLab"]
tmst_LabMat=-coef(lm1)["qMat"] / coef(lm1)["qLab"]
tmst_MatCap=-coef(lm1)["qCap"] / coef(lm1)["qMat"]
tmst_MatLab=-coef(lm1)["qLab"] / coef(lm1)["qMat"]

### Taux Marginal relatif de substitution technique 
data[,tmrst_CapLab:=-eps_lab/eps_cap]
data[,tmrst_CapMat:=-eps_mat/eps_cap]
data[,tmrst_LabCap:=-eps_cap/eps_lab]
data[,tmrst_LabMat:=-eps_mat/eps_lab]
data[,tmrst_MatCap:=-eps_cap/eps_mat]
data[,tmrst_MatLab:=-eps_lab/eps_mat]
colMeans(data[, c("tmrst_CapLab","tmrst_MatLab"), with=FALSE])
```






**Normalité des résidus** 
```{r,warning=FALSE, comment=NA, message=FALSE}
plot(lm1)
```

On remarque que le nuage de point s'éloigne de la ligne pointillé

Le test de Shapiro-Wilk peut également être employé pour évaluer la normalité des résidus. L'hypothèse de normalité est rejetée si la p-value est inférieure à 0.05.

```{r,warning=FALSE, comment=NA, message=FALSE}
shapiro.test(residuals(lm1))
```

##Cobb-Douglass
$$q_i = A\prod_{k=1}^{3}x_{ik}^{\alpha k}\epsilon_i $$
```{r,warning=FALSE, comment=NA, message=FALSE}
lm2<-lm(log(qOut)~log(qLab)+log(qMat)+log(qCap), data=data)
stargazer(lm2, type="text")
```

La p-value est inférieur à 0.05 on rejette donc l'hypothèse de normalité de résidus. 

```{r,warning=FALSE, comment=NA, message=FALSE}
tab1<-vcov(lm2)
```

Calcul de la somme des 3 variances des facteurs de production. Pour pouvoir faire le test des rendements d'échelle.

$$V(X + Y + Z) = V(X) + V(Y) + V(Z) + 2Cov(X,Y) + 2COV(X,Z) + 2COV(Y,Z)$$


```{r,warning=FALSE, comment=NA, message=FALSE}
Var<-tab1[2,2]+ tab1[3,3] + tab1[4,4] + 2*tab1[3,2] + 2*tab1[4,2] + 2*tab1[4,3]
Var
seescd<-sqrt(Var)
```

On fait le test pour savoir si cette valeur est significativement différent de 1 pour savoir si on a des rendements constants. Les rendements sont donc croissants. 



OU (autre méthode)

```{r,warning=FALSE, comment=NA, message=FALSE}
# Intervalle des rendements d'echelle
shapiro.test(residuals(lm2))
vcov(lm2)
est=sum(lm2$coefficients[-1])
dESCD=c( 0, 1, 1, 1 )
varESCD=t(dESCD) %*% vcov(lm2) %*% dESCD
seESCD=sqrt( varESCD )
est+1.96*seESCD
est-1.96*seESCD
```

**1 n'est pas dans l'intervalle ce qui signifie que les rendements d'echelle ne sont pas constants. **


##Translog

$$q_i = \alpha + \sum_{k=1}^{3} \beta_k lnx_{ik} + \frac{1}{2} \sum_{l=1}^{3}\sum_{k=1}^{3} \beta_{kl}lnx_{ik}lnx_{il}  +\epsilon_i$$

```{r,warning=FALSE, comment=NA, message=FALSE}
lm3<-lm(log(qOut)~(log(qLab)+log(qMat)+log(qCap))^2+I(log(qMat)^2)+I(log(qCap)^2)+I(log(qLab)^2), data=data)
stargazer(lm3, type="text")
```

Très grosse colinéarité il faut faire vif test !

 Test entre Cobb Douglass et Translog
A faire test de Fisher où $H_0$ est le modele contraint (Cobb Douglass) et $H_1$ non contraint (Translog) 

```{r,warning=FALSE, comment=NA, message=FALSE}
require(car)
vif(lm3)
```

Fonction pour faire le test manuellement : 

```{r}
f_test=function(contraint,non_contraint,seuil){
  rcontraint=summary(contraint)$r.squared
  rnoncontraint=summary(non_contraint)$r.squared
  Q=length(non_contraint$coefficients)-length(contraint$coefficients)
  resultat=((rnoncontraint-rcontraint)/(1-rnoncontraint))*(non_contraint$df.residual/Q)
  alpha=1-seuil
  F_theo=qf(alpha,Q,non_contraint$df.residual)
  if (resultat>F_theo){
    print(paste(round(resultat,3),"est strictement supérieur à la statistique théorique de Fisher, on rejette l'hypothèse nulle",sep=" "))
  }else {
    print(paste(round(resultat,3),"est inférieur à la statistique théorique de Fisher, on ne peux pas rejeter l'hypothèse nulle",sep=" "))
  }
  
}
```

```{r}
f_test(lm2,lm3,0.05)
```


## Quadratique
$$q_i = \alpha + \sum_{k=1}^{3} \beta_k x_{ik} + \frac{1}{2} \sum_{l=1}^{3}\sum_{k=1}^{3} \beta_{kl}x_{ik}x_{il}  +\epsilon_i $$
```{r}
prodQuad <- lm( qOut ~ qCap + qLab + qMat + I( 0.5 * qCap^2 ) + I( 0.5 * qLab^2 ) + I( 0.5 * qMat^2 )+ I( qCap * qLab ) + I( qCap * qMat ) + I( qLab * qMat ), data = data )
stargazer(prodQuad,type='text')
```

##CES

```{r}
lmces_CapLab=lm(log(qCap/qLab) ~ log(pLab/pCap), data=data)
lmces_CapMat=lm(log(qCap/qMat) ~ log(pMat/pCap), data=data)
lmces_LabCap=lm(log(qLab/qCap) ~ log(pCap/pLab), data=data)
lmces_LabMat=lm(log(qLab/qMat) ~ log(pMat/pLab), data=data)
lmces_MatCap=lm(log(qMat/qCap) ~ log(pCap/pMat), data=data)
lmces_MatLab=lm(log(qMat/qLab) ~ log(pLab/pMat), data=data)
ces_list=c("lmces_CapLab","lmces_CapMat","lmces_LabCap","lmces_LabMat","lmces_MatCap","lmces_MatLab")
for (i in ces_list){
  stargazer(get(i),type="text")
}
rm(i)

```


#Les fonctions de cout


##Cobb Douglass

$$c_i = A	\prod_{k=1}^{3} p_{ik}^{\alpha k} q_{i}^{\alpha y} \epsilon_i$$


```{r}
reg_cout_CD <- lm( log( cost ) ~ log( pCap ) + log( pLab ) + log( pMat ) + log( qOut ), data = data )
stargazer(reg_cout_CD, type='text')
```

On remarque que le capital n'est significatif qu'à 10% tandis que les autres facteurs de productions sont significatifs à 1%.

## Cobb-Douglass de court terme



$$c_i = A  x_{i3}^{\alpha3}	\prod_{k=1}^{2} p_{ik}^{\alpha k} q_{i}^{\alpha y} \epsilon_i)$$

```{r,warning=FALSE, comment=NA, message=FALSE}
require(stargazer)
costCDSR <- lm( log( vCost ) ~ log( pLab ) + log( pMat ) + log( qCap ) + log( qOut ), data = data )
stargazer(costCDSR, type='text')
```


##Translog

$$ln c_i = \alpha + \sum_{k=1}^{3} \beta_k + ln p_{ik} + \alpha_q ln(q_i)+ \frac{1}{2} \sum_{l=1}^{3}\sum_{k=1}^{3} \beta_{kl}lnp_{ik}lnp_{il} + \frac{1}{2} \alpha_{qq} (ln(q_i))² + \frac{1}{2} \sum_{k=1}^{3}\alpha_{kq}lnp_{ik}lnq_{i} + \epsilon_i    $$


```{r,warning=FALSE, comment=NA, message=FALSE}
costTL <- lm( log( cost ) ~ log( pCap ) + log( pLab ) + log( pMat ) + log( qOut ) + I( 0.5 * log( pCap )^2 ) + I( 0.5 * log( pLab )^2 ) + I( 0.5 * log( pMat )^2 ) + I( log( pCap ) * log( pLab ) ) + I( log( pCap ) * log( pMat ) ) + I( log( pLab ) * log( pMat ) ) + I( 0.5 * log( qOut )^2 ) + I( log( pCap ) * log( qOut ) ) + I( log( pLab ) * log( qOut ) ) + I( log( pMat ) * log( qOut ) ), data = data )
stargazer(costTL,type='text')
```



