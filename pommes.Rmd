---
title: "Étude de la production de pommes"
author: "Maurice Thomas & Thiret Juliette"
date: "30 octobre 2019"
output: html_document
---

```{r setup, include=FALSE,warning=FALSE, comment=NA, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r,echo=FALSE, message=FALSE,warning=FALSE}
require(data.table)
require(corrplot)
require(ggplot2)
require(lmtest)
require(miscTools)
require(stargazer)
require(frontier)
require(questionr)
```

```{r}
setwd("C:/Users/PDX/Desktop/Cours/M2/Mesure et perf/projetsco")
data=read.csv("Pommes.csv",sep=";",dec=",")
data=as.data.table(data)
```


#Manipulation de la base de données
```{r,warning=FALSE, comment=NA, message=FALSE}
data<-data[,-1]
```

```{r,warning=FALSE, comment=NA, message=FALSE}
summary(data)
str(data)
```

Toutes les variables sont bien codées.

```{r,warning=FALSE, comment=NA, message=FALSE, echo=TRUE}
data$adv<-as.factor(data$adv)
```


**Création de variables : les quantités de facteurs**

```{r,warning=FALSE, comment=NA, message=FALSE,echo=TRUE}
data$qCap<-(data$vCap/data$pCap)
data$qMat<-(data$vMat/data$pMat)
data$qLab<-(data$vLab/data$pLab)
```


**Calcul des productions moyennes de chaque input**

```{r,comment=NA}
data[,apCap:=qOut/qCap]
data[,apLab:=qOut/qLab]
data[,apMat:=qOut/qMat]

```


Vérification du cout total

```{r,warning=FALSE, comment=NA,echo=TRUE}
#cout total
data[,cost:=vCap+vLab+vMat]
all.equal(data$cost, with( data, pCap * qCap + pLab * qLab + pMat * qMat )) 
#cout variable
data[,vCost:=vLab+vMat]

```


#Statistiques descriptives


On remarque que beaucoup de variables sont corrélées entre elles. Mais ces corrélations semblent logiques


**\'{E}tude des couts de production**

Part des couts dans le cout total

```{r}
data$sh_Lab<-round(data$vLab/data$cost,2)
data$sh_Mat<-round(data$vMat/data$cost,2)
data$sh_Cap<-round(data$vCap/data$cost,2)

```

```{r}
boxplot(data$sh_Lab,data$sh_Cap,data$sh_Mat,names = c("Part du travail", "Part du capital", "Part des matériaux"),main="Part des inputs de production dans le cout total")
```

Part du travail est celle qui a une plus grande part dans la production, suivi des matériaux et du capital. 



```{r,comment=NA}
ggplot(data) +
  aes(x = data$qOut, y = data$sh_Lab) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du travail")
```

```{r,comment=NA}
ggplot(data) +
  aes(x = data$qOut, y = data$sh_Cap) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du Capital")
```

```{r,comment=NA}
ggplot(data) +
  aes(x = data$qOut, y = data$sh_Mat) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du Matériel")
```

On remarque que les intervalles de confiance sont très grand, ce qui signifie que pour quelques outliers (gros producteurs) biaisent la tendance général. Nous proposons donc de créer une variable qualitative qui prend soit la modalité petit_producteur et gros_producteur. Cela permettra d'avoir une analyse plus fine des parts des inputs dans la production.

*Création de la variable Taille_producteur*
```{r,comment=NA}
data$taille_producteur<-NA
data$taille_producteur <- ifelse(data$qOut < 3152400, "petit_producteur", "gros_producteur")
```


**Petit producteur vs gros producteur**
```{r,comment=NA}
ggplot(data[taille_producteur=="petit_producteur"]) +
  aes(x = qOut, y = sh_Mat) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du Matériel")
```
```{r,comment=NA}
ggplot(data[taille_producteur=="gros_producteur"]) +
  aes(x = qOut, y = sh_Mat) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du Matériel")
```


```{r,comment=NA}
ggplot(data[taille_producteur=="petit_producteur"]) +
  aes(x = qOut, y = sh_Cap) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du Capital")
```

```{r,comment=NA}
ggplot(data[taille_producteur=="gros_producteur"]) +
  aes(x = qOut, y = sh_Cap) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du Matériel")
```

```{r,comment=NA}
ggplot(data[taille_producteur=="petit_producteur"]) +
  aes(x = qOut, y = sh_Lab) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du ")
```

```{r,comment=NA}
ggplot(data[taille_producteur=="gros_producteur"]) +
  aes(x = qOut, y = sh_Lab) +
  geom_smooth() +
  geom_point() +
  xlab("Production totale") +
  ylab("Part du ")
```





**Etape 1: Histogrammes**

Représentez les histogrammes des productivités moyennes des facteurs de production de Cap,
Lab et Mat.

On considère donc les 3ieres colonnes

La productivité moyenne est : le nombre d'output / les moyens de production.
On créer donc une nouvelle colonne : vcap / qout par exemple.

```{r,warning=FALSE, comment=NA, message=FALSE}
data=as.data.table(data)
data=data[,pmcap := qOut/vCap]
```

```{r,warning=FALSE, comment=NA, message=FALSE}
data=as.data.table(data)
data=data[,pmlab := qOut/vLab]
```

```{r,warning=FALSE, comment=NA, message=FALSE}
data=as.data.table(data)
data=data[,pmmat := qOut/vMat]
```


**Histogrammes des productivités moyennes**
```{r,warning=FALSE, comment=NA, message=FALSE}
par(mfrow=c(1,1))
ggplot(data, aes(x=pmcap)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
ggplot(data, aes(x=pmlab)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
ggplot(data, aes(x=pmmat)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
```


**Etape 2** : Etude des corrélations.

Etudiez les corrélations entre les quantités des 3 facteurs de production utilisés dans ce secteur

Donnez la matrice des corrélations, et représentez graphiquement le nuage de point correspondant aux différents croisements des productivités moyennes prises deux à deux.
Représentez aussi les 3 séries de productivité moyenne par rapport à l'output total qOut.
###Matrice des corrélations
```{r,warning=FALSE, comment=NA, message=FALSE}
databis<-data[,c(1,2,3,6)]
M=cor(databis)
corrplot(M,type="lower", addCoef.col="red", method="shade")
```

###Nuage de point

```{r,warning=FALSE, comment=NA, message=FALSE}
par(mfrow=c(2,2))
ggplot(data, aes(x=pmlab, y=pmcap)) + 
  geom_point()+
  geom_smooth(method=lm)

ggplot(data, aes(x=pmlab, y=pmmat)) + 
  geom_point()+
  geom_smooth(method=lm)

ggplot(data, aes(x=pmcap, y=pmmat)) + 
  geom_point()+
  geom_smooth(method=lm)

```

```{r,warning=FALSE, comment=NA, message=FALSE}
attach(data)
par(mfrow=c(2,2))
ggplot(data, aes(x=pmlab, y=qOut)) + 
  geom_point()+
  geom_smooth(method=lm)

ggplot(data, aes(x=pmmat, y=qOut)) + 
  geom_point()+
  geom_smooth(method=lm)

ggplot(data, aes(x=pmcap, y=qOut)) + 
  geom_point()+
  geom_smooth(method=lm)

```


**Etape 3** : Calcul des indices de Pasches, Laspeyres et Fisher

Indice de valeur = indice de prix × indice de volume
indice de volume = indice de valeur / indice de prix


```{r,warning=FALSE, comment=NA, message=FALSE,echo=TRUE}
data[,XP:=(vCap+vLab+vMat) / (mean(qCap)*pCap + mean(qLab)*pLab + mean(qMat)*pMat)]
data[,XL:=(qCap*mean(pCap) + qLab*mean(pLab) + qMat*mean(pMat)) /
       (mean(qCap)*mean(pCap) + mean(qLab)*mean(pLab) + mean(qMat)*mean(pMat))]
data[,XF:=sqrt(XP*XL)]

```



**Etape 4** : Calcul d'un indice de productivité globale des facteurs


```{r,warning=FALSE, comment=NA,echo=TRUE}
data[,PGF:= qOut/XF]
```

```{r}
ggplot(data, aes(x=PGF, y=qOut, shape=taille_producteur, color=taille_producteur)) +
  geom_point()
```

**Etape 5** : Productivité globale et relations avec les quantités de facteurs

Représentez l'histogramme de l'indice de productivité globale des facteurs, sa relation avec l'output, sa relation avec les indices de quantité de facteur de production

```{r,warning=FALSE, comment=NA,echo=TRUE}
ggplot(data, aes(x=PGF)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
```

```{r,warning=FALSE, comment=NA, message=FALSE}
ggplot(data, aes(x=PGF, y=qOut)) + 
  geom_point()+
  geom_smooth(method=lm)
ggplot(data, aes(x=PGF, y=qCap)) + 
  geom_point()+
  geom_smooth(method=lm)
ggplot(data, aes(x=PGF, y=qLab)) + 
  geom_point()+
  geom_smooth(method=lm)
ggplot(data, aes(x=PGF, y=qMat)) + 
  geom_point()+
  geom_smooth(method=lm)
```

**Etape 6** : Boxplots

Représentez graphiquement (Boxplot) la productivité globale des facteurs pour les producteurs prenant conseil ou non.

```{r,warning=FALSE, comment=NA, message=FALSE}
#Box plots basiques
p <- ggplot(data, aes(x=adv, y=PGF,color=adv)) + 
  geom_boxplot()
p 
```


```{r}
tab<-table(data$taille_producteur, data$adv)
tab
```


```{r}
barplot(cprop(tab, total = FALSE), main = "Prise de conseil selon l'échelle de production")

```


**Quelles sont les entreprises les plus performantes aux vues statistiques descriptives.**

=> cf feuille


#Estimation des fonctions de production linéaire


## Linéaire 

$$q_i = \alpha + \sum_{k=1}^{3} \beta_k x_ik + \epsilon_i$$

Avec une fonction de production linéaire on suppose implicitement que les facteurs de production sont des substituts. 

```{r,warning=FALSE, comment=NA, message=FALSE}
require(stargazer)
lm1<-lm(qOut~qLab+qMat+qCap, data=data)
stargazer(lm1, type="text")
```



Obtenez-vous une fonction de production monotone croissante ?

```{r,warning=FALSE, comment=NA, message=FALSE}
plot(lm1$fitted.values, main="production estimée par la RLS")# estimée
abline(lm1)
```

La production trouvée n'est pas monotone croissante. 

** Le coefficient associé au facteur de production du capital est il significatif ? **

La quantité de capitale n'est pas significative. Les autres quantités sont significatives au seuil de 1%. 

**Peut-on dire que chaque facteur de production est essentiel ?**

Oui chaque facteur est essentiel à la production ce n'est pas parce que le coefficient du capital n'est pas significatif que l'on doit en déduire cela. Il faut surtout estimer une autre fonction de production. 


**Comparer à l'aide d'un graphique la production observée et la production estimée **

```{r,warning=FALSE, comment=NA, message=FALSE}
compPlot(data$qOut, fitted(lm1))
sum(fitted(lm1) < 0 )
coef(lm1)
```


**Vérifier si la production est toujours positive **

```{r,warning=FALSE, comment=NA, message=FALSE}

sum(lm1$fitted.values< 0)
sort(lm1$fitted.values)[1:5]

```
Il y a une production estimée qui est négative

### Les élasticités

<ul>
<li>Calculez pour chaque firme l'élasticité de l'output par rapport à chacun des trois inputs capital,travail et matériaux.</li>
<li>Calculer la moyenne de ces élasticités sur l'échantillon.</li>
<li>Représentez la distribution empirique des ces élasticités. </li>
</ul>

$$\epsilon_i =  \frac{\partial f(x)}{\partial x_i} \frac {x_i}{f(x)} $$

```{r,message=FALSE, warning=FALSE, comment=NA}
data[,eps_cap:=coef(lm1)["qCap"]*qCap/qOut]
data[,eps_lab:=coef(lm1)["qLab"]*qCap/qOut]
data[,eps_mat:=coef(lm1)["qMat"]*qCap/qOut]

```

```{r}
# Elasticités
data[,eps_cap:=coef(lm1)["qCap"]*qCap/qOut]
data[,eps_lab:=coef(lm1)["qLab"]*qLab/qOut]
data[,eps_mat:=coef(lm1)["qMat"]*qMat/qOut]
colMeans(data[, c("eps_cap","eps_lab","eps_mat"), with=FALSE])
ggplot(data, aes(x=eps_cap)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
ggplot(data, aes(x=eps_lab)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")
ggplot(data, aes(x=eps_mat)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")

# Comparaison avec la production estimée
data[,eps_cap_fit:=coef(lm1)["qCap"]*qCap/fitted(lm1)]
data[,eps_lab_fit:=coef(lm1)["qLab"]*qLab/fitted(lm1)]
data[,eps_mat_fit:=coef(lm1)["qMat"]*qMat/fitted(lm1)]
colMeans(data[, c("eps_cap_fit","eps_lab_fit","eps_mat_fit"), with=FALSE])
ggplot(data, aes(x=eps_cap_fit)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité estimée du capital")
ggplot(data, aes(x=eps_lab_fit)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité estimée du travail")
ggplot(data, aes(x=eps_mat_fit)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité estimée du matériel")

#Elasticités d'échelle
data[,eps_echelle:= eps_cap+eps_lab+eps_mat]
data[,eps_echelle_fit:= eps_cap_fit+eps_lab_fit+eps_mat_fit]
colMeans(data[, c("eps_echelle","eps_echelle_fit"), with=FALSE])
ggplot(data, aes(x=eps_echelle)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité d'échelle")

ggplot(data, aes(x=eps_echelle_fit)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666")+
  ggtitle("élasticité d'échelle estimée")
```

```{r,warning=FALSE, comment=NA, message=FALSE}
### Taux marginal de substitution technique
tmst_CapLab=-coef(lm1)["qLab"] / coef(lm1)["qCap"]
tmst_CapMat=-coef(lm1)["qMat"] / coef(lm1)["qCap"]
tmst_LabCap=-coef(lm1)["qCap"] / coef(lm1)["qLab"]
tmst_LabMat=-coef(lm1)["qMat"] / coef(lm1)["qLab"]
tmst_MatCap=-coef(lm1)["qCap"] / coef(lm1)["qMat"]
tmst_MatLab=-coef(lm1)["qLab"] / coef(lm1)["qMat"]

### Taux Marginal relatif de substitution technique 
data[,tmrst_CapLab:=-eps_lab/eps_cap]
data[,tmrst_CapMat:=-eps_mat/eps_cap]
data[,tmrst_LabCap:=-eps_cap/eps_lab]
data[,tmrst_LabMat:=-eps_mat/eps_lab]
data[,tmrst_MatCap:=-eps_cap/eps_mat]
data[,tmrst_MatLab:=-eps_lab/eps_mat]
colMeans(data[, c("tmrst_CapLab","tmrst_MatLab"), with=FALSE])
```


**Normalité des résidus** 
```{r,warning=FALSE, comment=NA, message=FALSE}
plot(lm1)
```

On remarque que le nuage de point s'éloigne de la ligne pointillé

Le test de Shapiro-Wilk peut également être employé pour évaluer la normalité des résidus. L'hypothèse de normalité est rejetée si la p-value est inférieure à 0.05.

```{r,warning=FALSE, comment=NA, message=FALSE}
shapiro.test(residuals(lm1))
```

##Cobb-Douglass

$$q_i = A\prod_{k=1}^{3}x_{ik}^{\alpha k}\epsilon_i + w $$


```{r,warning=FALSE, comment=NA, message=FALSE}
lm2<-lm(log(qOut)~log(qLab)+log(qMat)+log(qCap), data=data)
stargazer(lm2, type="text")
```


La p-value est inférieur à 0.05 on rejette donc l'hypothèse de normalité de résidus. 
**Calcul des productivité marginale de chaque input**
```{r,comment=NA}
data[,mpCap:=coef(lm2)["log(qCap)"]*apCap]
data[,mpLab:=coef(lm2)["log(qLab)"]*apLab]
data[,mpMat:=coef(lm2)["log(qMat)"]*apMat]

```

**TMST : Taux marginale de substitution technique**

```{r,comment=NA}
 data$TMST_CapLabCD <- - data$mpLab / data$mpCap
 data$TMST_LabCapCD <- - data$mpCap / data$mpLab
 data$TMST_CapMatCD <- - data$mpMat / data$mpCap
 data$TMST_MatCapCD <- - data$mpCap / data$mpMat
 data$TMST_LabMatCD <- - data$mpMat / data$mpLab
 data$TMST_MatLabCD <- - data$mpLab / data$mpMat
```

```{r,comment=NA}
colMeans(data[, c("TMST_CapLabCD","TMST_CapMatCD","TMST_LabCapCD",
                  "TMST_LabMatCD","TMST_MatCapCD","TMST_MatLabCD"), with=FALSE])

```


Le prix du capital en unité de matériaux et de 15, le prix du cap en unité de travail est de 2. Le prix du travail en unité de matériaux est de 9, cette analyse de prix de chaque input en unité des autres inputs nous informent que le meilleur input est le matériel le deuxième est le travail et enfin le dernier le capital. Notre critère de hierarchisation prend en compte les productivités moyennes, les productivité marginales et les taux marginaux de substitution techniques des 3 inputs. 

```{r,warning=FALSE, comment=NA, message=FALSE}
tab1<-vcov(lm2)
```

Calcul de la somme des 3 variances des facteurs de production. Pour pouvoir faire le test des rendements d'échelle.

$$V(X + Y + Z) = V(X) + V(Y) + V(Z) + 2Cov(X,Y) + 2COV(X,Z) + 2COV(Y,Z)$$


```{r,warning=FALSE, comment=NA, message=FALSE}
Var<-tab1[2,2]+ tab1[3,3] + tab1[4,4] + 2*tab1[3,2] + 2*tab1[4,2] + 2*tab1[4,3]
Var
seescd<-sqrt(Var)
```

On fait le test pour savoir si cette valeur est significativement différent de 1 pour savoir si on a des rendements constants. Les rendements sont donc croissants. 



OU (autre méthode)

```{r,warning=FALSE, comment=NA, message=FALSE}
# Intervalle des rendements d'echelle
shapiro.test(residuals(lm2))
vcov(lm2)
est=sum(lm2$coefficients[-1])
dESCD=c( 0, 1, 1, 1 )
varESCD=t(dESCD) %*% vcov(lm2) %*% dESCD
seESCD=sqrt( varESCD )
est+1.96*seESCD
est-1.96*seESCD
```

Les rendements d'échelle pour la cobb Douglass sont égaux à 1.46.
On fait un test pour savoir s'ils sont significativement différents de 1, donc afin de voir s'ils sont croissants ou non. 
Or
**1 n'est pas dans l'intervalle ce qui signifie que les rendements d'echelle ne sont pas constants. **


##Translog

$$q_i = \alpha + \sum_{k=1}^{3} \beta_k lnx_{ik} + \frac{1}{2} \sum_{l=1}^{3}\sum_{k=1}^{3} \beta_{kl}lnx_{ik}lnx_{il}  +\epsilon_i$$

```{r,warning=FALSE, comment=NA, message=FALSE}
lm3<-lm(log(qOut)~(log(qLab)+log(qMat)+log(qCap))^2+I(log(qMat)^2)+I(log(qCap)^2)+I(log(qLab)^2), data=data)
stargazer(lm3, type="text")
```

Très grosse colinéarité il faut faire vif test !

 Test entre Cobb Douglass et Translog
A faire test de Fisher où $H_0$ est le modele contraint (Cobb Douglass) et $H_1$ non contraint (Translog) 

```{r,warning=FALSE, comment=NA, message=FALSE}
require(car)
vif(lm3)
```

Fonction pour faire le test manuellement : 

```{r,comment=NA}
f_test=function(contraint,non_contraint,seuil){
  rcontraint=summary(contraint)$r.squared
  rnoncontraint=summary(non_contraint)$r.squared
  Q=length(non_contraint$coefficients)-length(contraint$coefficients)
  resultat=((rnoncontraint-rcontraint)/(1-rnoncontraint))*(non_contraint$df.residual/Q)
  alpha=1-seuil
  F_theo=qf(alpha,Q,non_contraint$df.residual)
  if (resultat>F_theo){
    print(paste(round(resultat,3),"est strictement supérieur à la statistique théorique de Fisher, on rejette l'hypothèse nulle",sep=" "))
  }else {
    print(paste(round(resultat,3),"est inférieur à la statistique théorique de Fisher, on ne peux pas rejeter l'hypothèse nulle",sep=" "))
  }
  
}
```



```{r,comment=NA}
f_test(lm2,lm3,0.05)
```


## Quadratique
$$q_i = \alpha + \sum_{k=1}^{3} \beta_k x_{ik} + \frac{1}{2} \sum_{l=1}^{3}\sum_{k=1}^{3} \beta_{kl}x_{ik}x_{il}  +\epsilon_i $$
```{r,comment=NA}
prodQuad <- lm( qOut ~ qCap + qLab + qMat + I( 0.5 * qCap^2 ) + I( 0.5 * qLab^2 ) + I( 0.5 * qMat^2 )+ I( qCap * qLab ) + I( qCap * qMat ) + I( qLab * qMat ), data = data )
stargazer(prodQuad,type='text')
```

**Comparaison avec la fonction de production linéaire**
Pour comparer ces 2 fonctions nous pouvons faire un test du ratio du vraisemblance.

Il permet de comparer 2 modèles dont l'un est le cas particulier de l'autre. Ici le cas particulier est la linéaire et le modèle quadratique est l'extension du modèle linéaire. L'hypothèse nulle signifie que le modèle contraint est meilleur que le modèle non contraint. Si l'hypothèse nulle est rejetée alors le modèle non contraint apporte une meilleure explication de la production que le modèle non contraint


```{r,comment=NA}
lrtest( lm1, prodQuad )
```

La p-value est inférieure à 0.01 donc on rejette l'hypothèse nulle, le modèle quadratique apporte une information significative par rapport au modèle linéaire.  

##CES

```{r,comment=NA}
lmces_CapLab=lm(log(qCap/qLab) ~ log(pLab/pCap), data=data)
lmces_CapMat=lm(log(qCap/qMat) ~ log(pMat/pCap), data=data)
lmces_LabCap=lm(log(qLab/qCap) ~ log(pCap/pLab), data=data)
lmces_LabMat=lm(log(qLab/qMat) ~ log(pMat/pLab), data=data)
lmces_MatCap=lm(log(qMat/qCap) ~ log(pCap/pMat), data=data)
lmces_MatLab=lm(log(qMat/qLab) ~ log(pLab/pMat), data=data)
ces_list=c("lmces_CapLab","lmces_CapMat","lmces_LabCap","lmces_LabMat","lmces_MatCap","lmces_MatLab")
for (i in ces_list){
  stargazer(get(i),type="text")
}
rm(i)

```


Faire des tests de Student. 

#Les fonctions de cout


##Cobb Douglass

$$c_i = A	\prod_{k=1}^{3} p_{ik}^{\alpha k} q_{i}^{\alpha y} \epsilon_i$$


```{r,comment=NA}
reg_cout_CD <- lm( log( cost ) ~ log( pCap ) + log( pLab ) + log( pMat ) + log( qOut ), data = data )
stargazer(reg_cout_CD, type='text')
```

On remarque que le capital n'est significatif qu'à 10% tandis que les autres facteurs de productions sont significatifs à 1%.

## Cobb-Douglass de court terme



$$c_i = A  x_{i3}^{\alpha3}	\prod_{k=1}^{2} p_{ik}^{\alpha k} q_{i}^{\alpha y} \epsilon_i)$$

```{r,warning=FALSE, comment=NA, message=FALSE}
require(stargazer)
costCDSR <- lm( log( vCost ) ~ log( pLab ) + log( pMat ) + log( qCap ) + log( qOut ), data = data )
stargazer(costCDSR, type='text')
```


##Translog

$$ln c_i = \alpha + \sum_{k=1}^{3} \beta_k + ln p_{ik} + \alpha_q ln(q_i)+ \frac{1}{2} \sum_{l=1}^{3}\sum_{k=1}^{3} \beta_{kl}lnp_{ik}lnp_{il} + \frac{1}{2} \alpha_{qq} (ln(q_i))² + \frac{1}{2} \sum_{k=1}^{3}\alpha_{kq}lnp_{ik}lnq_{i} + \epsilon_i    $$


```{r,warning=FALSE, comment=NA, message=FALSE}
costTL <- lm( log( cost ) ~ log( pCap ) + log( pLab ) + log( pMat ) + log( qOut ) + I( 0.5 * log( pCap )^2 ) + I( 0.5 * log( pLab )^2 ) + I( 0.5 * log( pMat )^2 ) + I( log( pCap ) * log( pLab ) ) + I( log( pCap ) * log( pMat ) ) + I( log( pLab ) * log( pMat ) ) + I( 0.5 * log( qOut )^2 ) + I( log( pCap ) * log( qOut ) ) + I( log( pLab ) * log( qOut ) ) + I( log( pMat ) * log( qOut ) ), data = data )
stargazer(costTL,type='text')
```


#Calcul des cout marginaux de production. dérivée du cout // à la production (il ne sera pas constant). 

$$\frac{\partial{logC}} {\partial{logQ}}   \frac{Q}{C}$$

Celle qui ont le plus grand écart cout et cout moyen. Cout moyen et cout marg doivent être decroissant si rendement échelle sont croissants. 

# Classement des entreprises de plusieurs façons. (par indice de prod, un autre par taux de marge, score basé sur les deux trucs précédents). 

#Objectifs finals: faire classement des entreprises. Faire des propositions. 

#Cobb douglass :  rendement echelle mais elasticité substitutions (allen = 1)
#CES généralisation de la cobb douglass

#Si c'est différent de 1 la technologie n'est pas cobb douglass. Normalement tout est significativement différent de 1. 

Part du cout du travail dans le cout total , de meme pour capital et matériaux. 
Part de depense relative des facteurs indépendante du niveau de production ? Part de dépenses relatives sont constantes ? 
Regarder distributions prx des inputs pour chaque producteurs, il y a une variabilité au niveau individuel.

#Faire tous les tests de significativité global. (à voir)


# MODELE SFA

##Cobb Douglass

$$Q= AK^{\alpha}L^{\beta}e^W$$
$$Q= AK^{\alpha}L^{\beta}e^{V-U}$$
$${\displaystyle V\sim {\mathcal {N}}(\mu ,\sigma ^{2})}.$$
$${\displaystyle U\sim {\mathcal {N^+}}(\mu ,\sigma ^{2})}.$$

$$log(Q) = constante + \alpha lnk + \beta ln L + V - U $$

```{r,comment=NA}
prodCDSFA<-sfa(log(qOut)~log(qCap) + log(qLab) + log(qMat), data= data)
summary(prodCDSFA)
```
Fonction qui caractèrise une technologie de prod sauf que la fonction ici représente la technologie la plus efficace. 

Dans fonction de prod et sfa c'est le max que l'on peut produire avec ce que l'on peut produire. 

Entre 0 et 1, écart entre production effective et ce qui est produit. (un ratio).

Fonction frontière de cout ou la le but est pas de classe les ent des ent qui produise le plus mais de classer celles qui ont le cout le plus faible.

Pour une fonction sfa de cout une entreprise inefficace va etre au dessus de la fonction frontière de cout
ineffDecrease = pour inverser fonction frontiere

